{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9ebd0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #문장을 단어별로 자르는것\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #채우는거\n",
    "from tensorflow.keras.utils import to_categorical #원핫인코딩 해주는함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e916128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "화려한 도시를 그리며 찾아왔네\n",
    "\n",
    "그 곳은 춥고도 험한 곳\n",
    "\n",
    "여기저기 헤매다 초라한 문턱에서\n",
    "\n",
    "뜨거운 눈물을 먹는다\n",
    "\n",
    "머나먼 길을 찾아 여기에 꿈을 찾아 여기에\n",
    "\n",
    "괴롭고도 험한 이 길을 왔는데\n",
    "\n",
    "이 세상 어디가 숲인지 어디가 늪인지\n",
    "\n",
    "그 누구도 말을 않네\n",
    "\n",
    "사람들은 저마다 고향을 찾아가네\n",
    "\n",
    "나는 지금 홀로 남아서\n",
    "\n",
    "빌딩 속을 헤매이다 초라한 골목에서\n",
    "\n",
    "뜨거운 눈물을 먹는다\n",
    "\n",
    "저기 저 별은 나의 마음을 알까 나의 꿈을 알까\n",
    "\n",
    "괴로울 땐 슬픈 노래를 부른다\n",
    "\n",
    "슬퍼질 땐 차라리 나 홀로 눈을 감고 싶어\n",
    "\n",
    "고향의 향기 들으면서\n",
    "\n",
    "저기 저 별은 나의 마음 알까 나의 꿈을 알까\n",
    "\n",
    "괴로울 땐 슬픈 노래를 부른다\n",
    "\n",
    "이 세상 어디가 숲인지 어디가 늪인지\n",
    "\n",
    "그 누구도 말을 않네\n",
    "\n",
    "슬퍼질 땐 차라리 나 홀로 눈을 감고 싶어\n",
    "\n",
    "고향의 향기 들으면서\n",
    "\n",
    "고향의 향기 들으면서\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "036e1354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "화려한 도시를 그리며 찾아왔네\n",
      "\n",
      "그 곳은 춥고도 험한 곳\n",
      "\n",
      "여기저기 헤매다 초라한 문턱에서\n",
      "\n",
      "뜨거운 눈물을 먹는다\n",
      "\n",
      "머나먼 길을 찾아 여기에 꿈을 찾아 여기에\n",
      "\n",
      "괴롭고도 험한 이 길을 왔는데\n",
      "\n",
      "이 세상 어디가 숲인지 어디가 늪인지\n",
      "\n",
      "그 누구도 말을 않네\n",
      "\n",
      "사람들은 저마다 고향을 찾아가네\n",
      "\n",
      "나는 지금 홀로 남아서\n",
      "\n",
      "빌딩 속을 헤매이다 초라한 골목에서\n",
      "\n",
      "뜨거운 눈물을 먹는다\n",
      "\n",
      "저기 저 별은 나의 마음을 알까 나의 꿈을 알까\n",
      "\n",
      "괴로울 땐 슬픈 노래를 부른다\n",
      "\n",
      "슬퍼질 땐 차라리 나 홀로 눈을 감고 싶어\n",
      "\n",
      "고향의 향기 들으면서\n",
      "\n",
      "저기 저 별은 나의 마음 알까 나의 꿈을 알까\n",
      "\n",
      "괴로울 땐 슬픈 노래를 부른다\n",
      "\n",
      "이 세상 어디가 숲인지 어디가 늪인지\n",
      "\n",
      "그 누구도 말을 않네\n",
      "\n",
      "슬퍼질 땐 차라리 나 홀로 눈을 감고 싶어\n",
      "\n",
      "고향의 향기 들으면서\n",
      "\n",
      "고향의 향기 들으면서\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "36578ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d25b0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f066f2cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'어디가': 1,\n",
       " '나의': 2,\n",
       " '알까': 3,\n",
       " '땐': 4,\n",
       " '그': 5,\n",
       " '꿈을': 6,\n",
       " '이': 7,\n",
       " '홀로': 8,\n",
       " '고향의': 9,\n",
       " '향기': 10,\n",
       " '들으면서': 11,\n",
       " '험한': 12,\n",
       " '초라한': 13,\n",
       " '뜨거운': 14,\n",
       " '눈물을': 15,\n",
       " '먹는다': 16,\n",
       " '길을': 17,\n",
       " '찾아': 18,\n",
       " '여기에': 19,\n",
       " '세상': 20,\n",
       " '숲인지': 21,\n",
       " '늪인지': 22,\n",
       " '누구도': 23,\n",
       " '말을': 24,\n",
       " '않네': 25,\n",
       " '저기': 26,\n",
       " '저': 27,\n",
       " '별은': 28,\n",
       " '괴로울': 29,\n",
       " '슬픈': 30,\n",
       " '노래를': 31,\n",
       " '부른다': 32,\n",
       " '슬퍼질': 33,\n",
       " '차라리': 34,\n",
       " '나': 35,\n",
       " '눈을': 36,\n",
       " '감고': 37,\n",
       " '싶어': 38,\n",
       " '화려한': 39,\n",
       " '도시를': 40,\n",
       " '그리며': 41,\n",
       " '찾아왔네': 42,\n",
       " '곳은': 43,\n",
       " '춥고도': 44,\n",
       " '곳': 45,\n",
       " '여기저기': 46,\n",
       " '헤매다': 47,\n",
       " '문턱에서': 48,\n",
       " '머나먼': 49,\n",
       " '괴롭고도': 50,\n",
       " '왔는데': 51,\n",
       " '사람들은': 52,\n",
       " '저마다': 53,\n",
       " '고향을': 54,\n",
       " '찾아가네': 55,\n",
       " '나는': 56,\n",
       " '지금': 57,\n",
       " '남아서': 58,\n",
       " '빌딩': 59,\n",
       " '속을': 60,\n",
       " '헤매이다': 61,\n",
       " '골목에서': 62,\n",
       " '마음을': 63,\n",
       " '마음': 64}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dce73db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 65\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1 #+1해준이유는 파이썬인덱스 0번부터인데 토큰화는 1번부터라서\n",
    "print('단어 집합의 크기 : %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b934ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences = list()\n",
    "#sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "521343c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n화려한 도시를 그리며 찾아왔네\\n\\n그 곳은 춥고도 험한 곳\\n\\n여기저기 헤매다 초라한 문턱에서\\n\\n뜨거운 눈물을 먹는다\\n\\n머나먼 길을 찾아 여기에 꿈을 찾아 여기에\\n\\n괴롭고도 험한 이 길을 왔는데\\n\\n이 세상 어디가 숲인지 어디가 늪인지\\n\\n그 누구도 말을 않네\\n\\n사람들은 저마다 고향을 찾아가네\\n\\n나는 지금 홀로 남아서\\n\\n빌딩 속을 헤매이다 초라한 골목에서\\n\\n뜨거운 눈물을 먹는다\\n\\n저기 저 별은 나의 마음을 알까 나의 꿈을 알까\\n\\n괴로울 땐 슬픈 노래를 부른다\\n\\n슬퍼질 땐 차라리 나 홀로 눈을 감고 싶어\\n\\n고향의 향기 들으면서\\n\\n저기 저 별은 나의 마음 알까 나의 꿈을 알까\\n\\n괴로울 땐 슬픈 노래를 부른다\\n\\n이 세상 어디가 숲인지 어디가 늪인지\\n\\n그 누구도 말을 않네\\n\\n슬퍼질 땐 차라리 나 홀로 눈을 감고 싶어\\n\\n고향의 향기 들으면서\\n\\n고향의 향기 들으면서\\n'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f48171ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '화려한 도시를 그리며 찾아왔네',\n",
       " '',\n",
       " '그 곳은 춥고도 험한 곳',\n",
       " '',\n",
       " '여기저기 헤매다 초라한 문턱에서',\n",
       " '',\n",
       " '뜨거운 눈물을 먹는다',\n",
       " '',\n",
       " '머나먼 길을 찾아 여기에 꿈을 찾아 여기에',\n",
       " '',\n",
       " '괴롭고도 험한 이 길을 왔는데',\n",
       " '',\n",
       " '이 세상 어디가 숲인지 어디가 늪인지',\n",
       " '',\n",
       " '그 누구도 말을 않네',\n",
       " '',\n",
       " '사람들은 저마다 고향을 찾아가네',\n",
       " '',\n",
       " '나는 지금 홀로 남아서',\n",
       " '',\n",
       " '빌딩 속을 헤매이다 초라한 골목에서',\n",
       " '',\n",
       " '뜨거운 눈물을 먹는다',\n",
       " '',\n",
       " '저기 저 별은 나의 마음을 알까 나의 꿈을 알까',\n",
       " '',\n",
       " '괴로울 땐 슬픈 노래를 부른다',\n",
       " '',\n",
       " '슬퍼질 땐 차라리 나 홀로 눈을 감고 싶어',\n",
       " '',\n",
       " '고향의 향기 들으면서',\n",
       " '',\n",
       " '저기 저 별은 나의 마음 알까 나의 꿈을 알까',\n",
       " '',\n",
       " '괴로울 땐 슬픈 노래를 부른다',\n",
       " '',\n",
       " '이 세상 어디가 숲인지 어디가 늪인지',\n",
       " '',\n",
       " '그 누구도 말을 않네',\n",
       " '',\n",
       " '슬퍼질 땐 차라리 나 홀로 눈을 감고 싶어',\n",
       " '',\n",
       " '고향의 향기 들으면서',\n",
       " '',\n",
       " '고향의 향기 들으면서',\n",
       " '']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e3230491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 40, 41, 42]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['화려한 도시를 그리며 찾아왔네'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6f35243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습에 사용할 샘플의 개수: 94\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'): #줄바꿈 문자를 기준으로 문장 토큰화\n",
    "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "print('학습에 사용할 샘플의 개수: %d'%len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d6a40dd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[39, 40],\n",
       " [39, 40, 41],\n",
       " [39, 40, 41, 42],\n",
       " [5, 43],\n",
       " [5, 43, 44],\n",
       " [5, 43, 44, 12],\n",
       " [5, 43, 44, 12, 45],\n",
       " [46, 47],\n",
       " [46, 47, 13],\n",
       " [46, 47, 13, 48],\n",
       " [14, 15],\n",
       " [14, 15, 16],\n",
       " [49, 17],\n",
       " [49, 17, 18],\n",
       " [49, 17, 18, 19],\n",
       " [49, 17, 18, 19, 6],\n",
       " [49, 17, 18, 19, 6, 18],\n",
       " [49, 17, 18, 19, 6, 18, 19],\n",
       " [50, 12],\n",
       " [50, 12, 7],\n",
       " [50, 12, 7, 17],\n",
       " [50, 12, 7, 17, 51],\n",
       " [7, 20],\n",
       " [7, 20, 1],\n",
       " [7, 20, 1, 21],\n",
       " [7, 20, 1, 21, 1],\n",
       " [7, 20, 1, 21, 1, 22],\n",
       " [5, 23],\n",
       " [5, 23, 24],\n",
       " [5, 23, 24, 25],\n",
       " [52, 53],\n",
       " [52, 53, 54],\n",
       " [52, 53, 54, 55],\n",
       " [56, 57],\n",
       " [56, 57, 8],\n",
       " [56, 57, 8, 58],\n",
       " [59, 60],\n",
       " [59, 60, 61],\n",
       " [59, 60, 61, 13],\n",
       " [59, 60, 61, 13, 62],\n",
       " [14, 15],\n",
       " [14, 15, 16],\n",
       " [26, 27],\n",
       " [26, 27, 28],\n",
       " [26, 27, 28, 2],\n",
       " [26, 27, 28, 2, 63],\n",
       " [26, 27, 28, 2, 63, 3],\n",
       " [26, 27, 28, 2, 63, 3, 2],\n",
       " [26, 27, 28, 2, 63, 3, 2, 6],\n",
       " [26, 27, 28, 2, 63, 3, 2, 6, 3],\n",
       " [29, 4],\n",
       " [29, 4, 30],\n",
       " [29, 4, 30, 31],\n",
       " [29, 4, 30, 31, 32],\n",
       " [33, 4],\n",
       " [33, 4, 34],\n",
       " [33, 4, 34, 35],\n",
       " [33, 4, 34, 35, 8],\n",
       " [33, 4, 34, 35, 8, 36],\n",
       " [33, 4, 34, 35, 8, 36, 37],\n",
       " [33, 4, 34, 35, 8, 36, 37, 38],\n",
       " [9, 10],\n",
       " [9, 10, 11],\n",
       " [26, 27],\n",
       " [26, 27, 28],\n",
       " [26, 27, 28, 2],\n",
       " [26, 27, 28, 2, 64],\n",
       " [26, 27, 28, 2, 64, 3],\n",
       " [26, 27, 28, 2, 64, 3, 2],\n",
       " [26, 27, 28, 2, 64, 3, 2, 6],\n",
       " [26, 27, 28, 2, 64, 3, 2, 6, 3],\n",
       " [29, 4],\n",
       " [29, 4, 30],\n",
       " [29, 4, 30, 31],\n",
       " [29, 4, 30, 31, 32],\n",
       " [7, 20],\n",
       " [7, 20, 1],\n",
       " [7, 20, 1, 21],\n",
       " [7, 20, 1, 21, 1],\n",
       " [7, 20, 1, 21, 1, 22],\n",
       " [5, 23],\n",
       " [5, 23, 24],\n",
       " [5, 23, 24, 25],\n",
       " [33, 4],\n",
       " [33, 4, 34],\n",
       " [33, 4, 34, 35],\n",
       " [33, 4, 34, 35, 8],\n",
       " [33, 4, 34, 35, 8, 36],\n",
       " [33, 4, 34, 35, 8, 36, 37],\n",
       " [33, 4, 34, 35, 8, 36, 37, 38],\n",
       " [9, 10],\n",
       " [9, 10, 11],\n",
       " [9, 10],\n",
       " [9, 10, 11]]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "db9e0408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 최대 길이: 9\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(_) for _ in sequences) #모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n",
    "print('샘플의 최대 길이: {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8cd3160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre') \n",
    "#패딩화 가장 긴 샘플을 기준으로 나머지 부족한만큼 채워줌 pre는 앞쪽에다 채워줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6a634a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0, 39, 40],\n",
       "       [ 0,  0,  0,  0,  0,  0, 39, 40, 41],\n",
       "       [ 0,  0,  0,  0,  0, 39, 40, 41, 42],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  5, 43],\n",
       "       [ 0,  0,  0,  0,  0,  0,  5, 43, 44],\n",
       "       [ 0,  0,  0,  0,  0,  5, 43, 44, 12],\n",
       "       [ 0,  0,  0,  0,  5, 43, 44, 12, 45],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 46, 47],\n",
       "       [ 0,  0,  0,  0,  0,  0, 46, 47, 13],\n",
       "       [ 0,  0,  0,  0,  0, 46, 47, 13, 48],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 14, 15],\n",
       "       [ 0,  0,  0,  0,  0,  0, 14, 15, 16],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 49, 17],\n",
       "       [ 0,  0,  0,  0,  0,  0, 49, 17, 18],\n",
       "       [ 0,  0,  0,  0,  0, 49, 17, 18, 19],\n",
       "       [ 0,  0,  0,  0, 49, 17, 18, 19,  6],\n",
       "       [ 0,  0,  0, 49, 17, 18, 19,  6, 18],\n",
       "       [ 0,  0, 49, 17, 18, 19,  6, 18, 19],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 50, 12],\n",
       "       [ 0,  0,  0,  0,  0,  0, 50, 12,  7],\n",
       "       [ 0,  0,  0,  0,  0, 50, 12,  7, 17],\n",
       "       [ 0,  0,  0,  0, 50, 12,  7, 17, 51],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  7, 20],\n",
       "       [ 0,  0,  0,  0,  0,  0,  7, 20,  1],\n",
       "       [ 0,  0,  0,  0,  0,  7, 20,  1, 21],\n",
       "       [ 0,  0,  0,  0,  7, 20,  1, 21,  1],\n",
       "       [ 0,  0,  0,  7, 20,  1, 21,  1, 22],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  5, 23],\n",
       "       [ 0,  0,  0,  0,  0,  0,  5, 23, 24],\n",
       "       [ 0,  0,  0,  0,  0,  5, 23, 24, 25],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 52, 53],\n",
       "       [ 0,  0,  0,  0,  0,  0, 52, 53, 54],\n",
       "       [ 0,  0,  0,  0,  0, 52, 53, 54, 55],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 56, 57],\n",
       "       [ 0,  0,  0,  0,  0,  0, 56, 57,  8],\n",
       "       [ 0,  0,  0,  0,  0, 56, 57,  8, 58],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 59, 60],\n",
       "       [ 0,  0,  0,  0,  0,  0, 59, 60, 61],\n",
       "       [ 0,  0,  0,  0,  0, 59, 60, 61, 13],\n",
       "       [ 0,  0,  0,  0, 59, 60, 61, 13, 62],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 14, 15],\n",
       "       [ 0,  0,  0,  0,  0,  0, 14, 15, 16],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 26, 27],\n",
       "       [ 0,  0,  0,  0,  0,  0, 26, 27, 28],\n",
       "       [ 0,  0,  0,  0,  0, 26, 27, 28,  2],\n",
       "       [ 0,  0,  0,  0, 26, 27, 28,  2, 63],\n",
       "       [ 0,  0,  0, 26, 27, 28,  2, 63,  3],\n",
       "       [ 0,  0, 26, 27, 28,  2, 63,  3,  2],\n",
       "       [ 0, 26, 27, 28,  2, 63,  3,  2,  6],\n",
       "       [26, 27, 28,  2, 63,  3,  2,  6,  3],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 29,  4],\n",
       "       [ 0,  0,  0,  0,  0,  0, 29,  4, 30],\n",
       "       [ 0,  0,  0,  0,  0, 29,  4, 30, 31],\n",
       "       [ 0,  0,  0,  0, 29,  4, 30, 31, 32],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 33,  4],\n",
       "       [ 0,  0,  0,  0,  0,  0, 33,  4, 34],\n",
       "       [ 0,  0,  0,  0,  0, 33,  4, 34, 35],\n",
       "       [ 0,  0,  0,  0, 33,  4, 34, 35,  8],\n",
       "       [ 0,  0,  0, 33,  4, 34, 35,  8, 36],\n",
       "       [ 0,  0, 33,  4, 34, 35,  8, 36, 37],\n",
       "       [ 0, 33,  4, 34, 35,  8, 36, 37, 38],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  9, 10],\n",
       "       [ 0,  0,  0,  0,  0,  0,  9, 10, 11],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 26, 27],\n",
       "       [ 0,  0,  0,  0,  0,  0, 26, 27, 28],\n",
       "       [ 0,  0,  0,  0,  0, 26, 27, 28,  2],\n",
       "       [ 0,  0,  0,  0, 26, 27, 28,  2, 64],\n",
       "       [ 0,  0,  0, 26, 27, 28,  2, 64,  3],\n",
       "       [ 0,  0, 26, 27, 28,  2, 64,  3,  2],\n",
       "       [ 0, 26, 27, 28,  2, 64,  3,  2,  6],\n",
       "       [26, 27, 28,  2, 64,  3,  2,  6,  3],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 29,  4],\n",
       "       [ 0,  0,  0,  0,  0,  0, 29,  4, 30],\n",
       "       [ 0,  0,  0,  0,  0, 29,  4, 30, 31],\n",
       "       [ 0,  0,  0,  0, 29,  4, 30, 31, 32],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  7, 20],\n",
       "       [ 0,  0,  0,  0,  0,  0,  7, 20,  1],\n",
       "       [ 0,  0,  0,  0,  0,  7, 20,  1, 21],\n",
       "       [ 0,  0,  0,  0,  7, 20,  1, 21,  1],\n",
       "       [ 0,  0,  0,  7, 20,  1, 21,  1, 22],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  5, 23],\n",
       "       [ 0,  0,  0,  0,  0,  0,  5, 23, 24],\n",
       "       [ 0,  0,  0,  0,  0,  5, 23, 24, 25],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 33,  4],\n",
       "       [ 0,  0,  0,  0,  0,  0, 33,  4, 34],\n",
       "       [ 0,  0,  0,  0,  0, 33,  4, 34, 35],\n",
       "       [ 0,  0,  0,  0, 33,  4, 34, 35,  8],\n",
       "       [ 0,  0,  0, 33,  4, 34, 35,  8, 36],\n",
       "       [ 0,  0, 33,  4, 34, 35,  8, 36, 37],\n",
       "       [ 0, 33,  4, 34, 35,  8, 36, 37, 38],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  9, 10],\n",
       "       [ 0,  0,  0,  0,  0,  0,  9, 10, 11],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  9, 10],\n",
       "       [ 0,  0,  0,  0,  0,  0,  9, 10, 11]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "aae05bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "x = sequences[:,:-1]\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "181082ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0, 39],\n",
       "       [ 0,  0,  0,  0,  0,  0, 39, 40],\n",
       "       [ 0,  0,  0,  0,  0, 39, 40, 41],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  5],\n",
       "       [ 0,  0,  0,  0,  0,  0,  5, 43],\n",
       "       [ 0,  0,  0,  0,  0,  5, 43, 44],\n",
       "       [ 0,  0,  0,  0,  5, 43, 44, 12],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 46],\n",
       "       [ 0,  0,  0,  0,  0,  0, 46, 47],\n",
       "       [ 0,  0,  0,  0,  0, 46, 47, 13],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 14],\n",
       "       [ 0,  0,  0,  0,  0,  0, 14, 15],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 49],\n",
       "       [ 0,  0,  0,  0,  0,  0, 49, 17],\n",
       "       [ 0,  0,  0,  0,  0, 49, 17, 18],\n",
       "       [ 0,  0,  0,  0, 49, 17, 18, 19],\n",
       "       [ 0,  0,  0, 49, 17, 18, 19,  6],\n",
       "       [ 0,  0, 49, 17, 18, 19,  6, 18],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 50],\n",
       "       [ 0,  0,  0,  0,  0,  0, 50, 12],\n",
       "       [ 0,  0,  0,  0,  0, 50, 12,  7],\n",
       "       [ 0,  0,  0,  0, 50, 12,  7, 17],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  7],\n",
       "       [ 0,  0,  0,  0,  0,  0,  7, 20],\n",
       "       [ 0,  0,  0,  0,  0,  7, 20,  1],\n",
       "       [ 0,  0,  0,  0,  7, 20,  1, 21],\n",
       "       [ 0,  0,  0,  7, 20,  1, 21,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  5],\n",
       "       [ 0,  0,  0,  0,  0,  0,  5, 23],\n",
       "       [ 0,  0,  0,  0,  0,  5, 23, 24],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 52],\n",
       "       [ 0,  0,  0,  0,  0,  0, 52, 53],\n",
       "       [ 0,  0,  0,  0,  0, 52, 53, 54],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 56],\n",
       "       [ 0,  0,  0,  0,  0,  0, 56, 57],\n",
       "       [ 0,  0,  0,  0,  0, 56, 57,  8],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 59],\n",
       "       [ 0,  0,  0,  0,  0,  0, 59, 60],\n",
       "       [ 0,  0,  0,  0,  0, 59, 60, 61],\n",
       "       [ 0,  0,  0,  0, 59, 60, 61, 13],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 14],\n",
       "       [ 0,  0,  0,  0,  0,  0, 14, 15],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 26],\n",
       "       [ 0,  0,  0,  0,  0,  0, 26, 27],\n",
       "       [ 0,  0,  0,  0,  0, 26, 27, 28],\n",
       "       [ 0,  0,  0,  0, 26, 27, 28,  2],\n",
       "       [ 0,  0,  0, 26, 27, 28,  2, 63],\n",
       "       [ 0,  0, 26, 27, 28,  2, 63,  3],\n",
       "       [ 0, 26, 27, 28,  2, 63,  3,  2],\n",
       "       [26, 27, 28,  2, 63,  3,  2,  6],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 29],\n",
       "       [ 0,  0,  0,  0,  0,  0, 29,  4],\n",
       "       [ 0,  0,  0,  0,  0, 29,  4, 30],\n",
       "       [ 0,  0,  0,  0, 29,  4, 30, 31],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 33],\n",
       "       [ 0,  0,  0,  0,  0,  0, 33,  4],\n",
       "       [ 0,  0,  0,  0,  0, 33,  4, 34],\n",
       "       [ 0,  0,  0,  0, 33,  4, 34, 35],\n",
       "       [ 0,  0,  0, 33,  4, 34, 35,  8],\n",
       "       [ 0,  0, 33,  4, 34, 35,  8, 36],\n",
       "       [ 0, 33,  4, 34, 35,  8, 36, 37],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  9],\n",
       "       [ 0,  0,  0,  0,  0,  0,  9, 10],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 26],\n",
       "       [ 0,  0,  0,  0,  0,  0, 26, 27],\n",
       "       [ 0,  0,  0,  0,  0, 26, 27, 28],\n",
       "       [ 0,  0,  0,  0, 26, 27, 28,  2],\n",
       "       [ 0,  0,  0, 26, 27, 28,  2, 64],\n",
       "       [ 0,  0, 26, 27, 28,  2, 64,  3],\n",
       "       [ 0, 26, 27, 28,  2, 64,  3,  2],\n",
       "       [26, 27, 28,  2, 64,  3,  2,  6],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 29],\n",
       "       [ 0,  0,  0,  0,  0,  0, 29,  4],\n",
       "       [ 0,  0,  0,  0,  0, 29,  4, 30],\n",
       "       [ 0,  0,  0,  0, 29,  4, 30, 31],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  7],\n",
       "       [ 0,  0,  0,  0,  0,  0,  7, 20],\n",
       "       [ 0,  0,  0,  0,  0,  7, 20,  1],\n",
       "       [ 0,  0,  0,  0,  7, 20,  1, 21],\n",
       "       [ 0,  0,  0,  7, 20,  1, 21,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  5],\n",
       "       [ 0,  0,  0,  0,  0,  0,  5, 23],\n",
       "       [ 0,  0,  0,  0,  0,  5, 23, 24],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 33],\n",
       "       [ 0,  0,  0,  0,  0,  0, 33,  4],\n",
       "       [ 0,  0,  0,  0,  0, 33,  4, 34],\n",
       "       [ 0,  0,  0,  0, 33,  4, 34, 35],\n",
       "       [ 0,  0,  0, 33,  4, 34, 35,  8],\n",
       "       [ 0,  0, 33,  4, 34, 35,  8, 36],\n",
       "       [ 0, 33,  4, 34, 35,  8, 36, 37],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  9],\n",
       "       [ 0,  0,  0,  0,  0,  0,  9, 10],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  9],\n",
       "       [ 0,  0,  0,  0,  0,  0,  9, 10]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "570302f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40, 41, 42, 43, 44, 12, 45, 47, 13, 48, 15, 16, 17, 18, 19,  6, 18,\n",
       "       19, 12,  7, 17, 51, 20,  1, 21,  1, 22, 23, 24, 25, 53, 54, 55, 57,\n",
       "        8, 58, 60, 61, 13, 62, 15, 16, 27, 28,  2, 63,  3,  2,  6,  3,  4,\n",
       "       30, 31, 32,  4, 34, 35,  8, 36, 37, 38, 10, 11, 27, 28,  2, 64,  3,\n",
       "        2,  6,  3,  4, 30, 31, 32, 20,  1, 21,  1, 22, 23, 24, 25,  4, 34,\n",
       "       35,  8, 36, 37, 38, 10, 11, 10, 11])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b3da14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "807b32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "78f3e393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e21c8d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3/3 - 1s - loss: 4.1813 - accuracy: 0.0106 - 792ms/epoch - 264ms/step\n",
      "Epoch 2/200\n",
      "3/3 - 0s - loss: 4.1607 - accuracy: 0.0106 - 20ms/epoch - 7ms/step\n",
      "Epoch 3/200\n",
      "3/3 - 0s - loss: 4.1423 - accuracy: 0.0213 - 10ms/epoch - 3ms/step\n",
      "Epoch 4/200\n",
      "3/3 - 0s - loss: 4.1243 - accuracy: 0.0638 - 439us/epoch - 146us/step\n",
      "Epoch 5/200\n",
      "3/3 - 0s - loss: 4.1045 - accuracy: 0.0851 - 19ms/epoch - 6ms/step\n",
      "Epoch 6/200\n",
      "3/3 - 0s - loss: 4.0847 - accuracy: 0.0745 - 11ms/epoch - 4ms/step\n",
      "Epoch 7/200\n",
      "3/3 - 0s - loss: 4.0635 - accuracy: 0.0745 - 3ms/epoch - 975us/step\n",
      "Epoch 8/200\n",
      "3/3 - 0s - loss: 4.0405 - accuracy: 0.0745 - 16ms/epoch - 5ms/step\n",
      "Epoch 9/200\n",
      "3/3 - 0s - loss: 4.0166 - accuracy: 0.1064 - 0s/epoch - 0s/step\n",
      "Epoch 10/200\n",
      "3/3 - 0s - loss: 3.9935 - accuracy: 0.1064 - 16ms/epoch - 5ms/step\n",
      "Epoch 11/200\n",
      "3/3 - 0s - loss: 3.9695 - accuracy: 0.1277 - 14ms/epoch - 5ms/step\n",
      "Epoch 12/200\n",
      "3/3 - 0s - loss: 3.9451 - accuracy: 0.1702 - 2ms/epoch - 680us/step\n",
      "Epoch 13/200\n",
      "3/3 - 0s - loss: 3.9218 - accuracy: 0.1702 - 14ms/epoch - 5ms/step\n",
      "Epoch 14/200\n",
      "3/3 - 0s - loss: 3.8949 - accuracy: 0.1277 - 1ms/epoch - 339us/step\n",
      "Epoch 15/200\n",
      "3/3 - 0s - loss: 3.8691 - accuracy: 0.1277 - 14ms/epoch - 5ms/step\n",
      "Epoch 16/200\n",
      "3/3 - 0s - loss: 3.8456 - accuracy: 0.1277 - 2ms/epoch - 760us/step\n",
      "Epoch 17/200\n",
      "3/3 - 0s - loss: 3.8171 - accuracy: 0.1596 - 14ms/epoch - 5ms/step\n",
      "Epoch 18/200\n",
      "3/3 - 0s - loss: 3.7893 - accuracy: 0.1596 - 7ms/epoch - 2ms/step\n",
      "Epoch 19/200\n",
      "3/3 - 0s - loss: 3.7614 - accuracy: 0.1915 - 8ms/epoch - 3ms/step\n",
      "Epoch 20/200\n",
      "3/3 - 0s - loss: 3.7303 - accuracy: 0.2447 - 0s/epoch - 0s/step\n",
      "Epoch 21/200\n",
      "3/3 - 0s - loss: 3.7013 - accuracy: 0.2553 - 16ms/epoch - 5ms/step\n",
      "Epoch 22/200\n",
      "3/3 - 0s - loss: 3.6710 - accuracy: 0.2553 - 12ms/epoch - 4ms/step\n",
      "Epoch 23/200\n",
      "3/3 - 0s - loss: 3.6407 - accuracy: 0.2660 - 3ms/epoch - 872us/step\n",
      "Epoch 24/200\n",
      "3/3 - 0s - loss: 3.6073 - accuracy: 0.2660 - 0s/epoch - 0s/step\n",
      "Epoch 25/200\n",
      "3/3 - 0s - loss: 3.5767 - accuracy: 0.2660 - 4ms/epoch - 1ms/step\n",
      "Epoch 26/200\n",
      "3/3 - 0s - loss: 3.5429 - accuracy: 0.2872 - 0s/epoch - 0s/step\n",
      "Epoch 27/200\n",
      "3/3 - 0s - loss: 3.5120 - accuracy: 0.3191 - 17ms/epoch - 6ms/step\n",
      "Epoch 28/200\n",
      "3/3 - 0s - loss: 3.4782 - accuracy: 0.3298 - 0s/epoch - 0s/step\n",
      "Epoch 29/200\n",
      "3/3 - 0s - loss: 3.4452 - accuracy: 0.3298 - 17ms/epoch - 6ms/step\n",
      "Epoch 30/200\n",
      "3/3 - 0s - loss: 3.4118 - accuracy: 0.3298 - 0s/epoch - 0s/step\n",
      "Epoch 31/200\n",
      "3/3 - 0s - loss: 3.3781 - accuracy: 0.3298 - 17ms/epoch - 6ms/step\n",
      "Epoch 32/200\n",
      "3/3 - 0s - loss: 3.3405 - accuracy: 0.3298 - 0s/epoch - 0s/step\n",
      "Epoch 33/200\n",
      "3/3 - 0s - loss: 3.3059 - accuracy: 0.3298 - 15ms/epoch - 5ms/step\n",
      "Epoch 34/200\n",
      "3/3 - 0s - loss: 3.2708 - accuracy: 0.3511 - 0s/epoch - 0s/step\n",
      "Epoch 35/200\n",
      "3/3 - 0s - loss: 3.2334 - accuracy: 0.3830 - 14ms/epoch - 5ms/step\n",
      "Epoch 36/200\n",
      "3/3 - 0s - loss: 3.1961 - accuracy: 0.4149 - 951us/epoch - 317us/step\n",
      "Epoch 37/200\n",
      "3/3 - 0s - loss: 3.1595 - accuracy: 0.4255 - 13ms/epoch - 4ms/step\n",
      "Epoch 38/200\n",
      "3/3 - 0s - loss: 3.1217 - accuracy: 0.4362 - 2ms/epoch - 638us/step\n",
      "Epoch 39/200\n",
      "3/3 - 0s - loss: 3.0845 - accuracy: 0.4362 - 0s/epoch - 0s/step\n",
      "Epoch 40/200\n",
      "3/3 - 0s - loss: 3.0451 - accuracy: 0.4681 - 4ms/epoch - 1ms/step\n",
      "Epoch 41/200\n",
      "3/3 - 0s - loss: 3.0077 - accuracy: 0.4894 - 0s/epoch - 0s/step\n",
      "Epoch 42/200\n",
      "3/3 - 0s - loss: 2.9690 - accuracy: 0.4894 - 7ms/epoch - 2ms/step\n",
      "Epoch 43/200\n",
      "3/3 - 0s - loss: 2.9301 - accuracy: 0.4894 - 6ms/epoch - 2ms/step\n",
      "Epoch 44/200\n",
      "3/3 - 0s - loss: 2.8933 - accuracy: 0.4894 - 5ms/epoch - 2ms/step\n",
      "Epoch 45/200\n",
      "3/3 - 0s - loss: 2.8529 - accuracy: 0.4894 - 0s/epoch - 0s/step\n",
      "Epoch 46/200\n",
      "3/3 - 0s - loss: 2.8140 - accuracy: 0.5000 - 17ms/epoch - 6ms/step\n",
      "Epoch 47/200\n",
      "3/3 - 0s - loss: 2.7751 - accuracy: 0.5106 - 0s/epoch - 0s/step\n",
      "Epoch 48/200\n",
      "3/3 - 0s - loss: 2.7371 - accuracy: 0.4894 - 14ms/epoch - 5ms/step\n",
      "Epoch 49/200\n",
      "3/3 - 0s - loss: 2.6994 - accuracy: 0.5000 - 0s/epoch - 0s/step\n",
      "Epoch 50/200\n",
      "3/3 - 0s - loss: 2.6595 - accuracy: 0.5000 - 15ms/epoch - 5ms/step\n",
      "Epoch 51/200\n",
      "3/3 - 0s - loss: 2.6214 - accuracy: 0.5000 - 611us/epoch - 204us/step\n",
      "Epoch 52/200\n",
      "3/3 - 0s - loss: 2.5843 - accuracy: 0.5000 - 13ms/epoch - 4ms/step\n",
      "Epoch 53/200\n",
      "3/3 - 0s - loss: 2.5470 - accuracy: 0.5000 - 3ms/epoch - 875us/step\n",
      "Epoch 54/200\n",
      "3/3 - 0s - loss: 2.5091 - accuracy: 0.5106 - 0s/epoch - 0s/step\n",
      "Epoch 55/200\n",
      "3/3 - 0s - loss: 2.4740 - accuracy: 0.5213 - 6ms/epoch - 2ms/step\n",
      "Epoch 56/200\n",
      "3/3 - 0s - loss: 2.4377 - accuracy: 0.5106 - 0s/epoch - 0s/step\n",
      "Epoch 57/200\n",
      "3/3 - 0s - loss: 2.4009 - accuracy: 0.5319 - 13ms/epoch - 4ms/step\n",
      "Epoch 58/200\n",
      "3/3 - 0s - loss: 2.3664 - accuracy: 0.5532 - 0s/epoch - 0s/step\n",
      "Epoch 59/200\n",
      "3/3 - 0s - loss: 2.3307 - accuracy: 0.5532 - 17ms/epoch - 6ms/step\n",
      "Epoch 60/200\n",
      "3/3 - 0s - loss: 2.2967 - accuracy: 0.5532 - 0s/epoch - 0s/step\n",
      "Epoch 61/200\n",
      "3/3 - 0s - loss: 2.2625 - accuracy: 0.5532 - 16ms/epoch - 5ms/step\n",
      "Epoch 62/200\n",
      "3/3 - 0s - loss: 2.2313 - accuracy: 0.5532 - 0s/epoch - 0s/step\n",
      "Epoch 63/200\n",
      "3/3 - 0s - loss: 2.1971 - accuracy: 0.5638 - 14ms/epoch - 5ms/step\n",
      "Epoch 64/200\n",
      "3/3 - 0s - loss: 2.1653 - accuracy: 0.5745 - 2ms/epoch - 631us/step\n",
      "Epoch 65/200\n",
      "3/3 - 0s - loss: 2.1305 - accuracy: 0.5851 - 13ms/epoch - 4ms/step\n",
      "Epoch 66/200\n",
      "3/3 - 0s - loss: 2.1009 - accuracy: 0.6064 - 3ms/epoch - 941us/step\n",
      "Epoch 67/200\n",
      "3/3 - 0s - loss: 2.0697 - accuracy: 0.6170 - 0s/epoch - 0s/step\n",
      "Epoch 68/200\n",
      "3/3 - 0s - loss: 2.0383 - accuracy: 0.6170 - 17ms/epoch - 6ms/step\n",
      "Epoch 69/200\n",
      "3/3 - 0s - loss: 2.0088 - accuracy: 0.6277 - 0s/epoch - 0s/step\n",
      "Epoch 70/200\n",
      "3/3 - 0s - loss: 1.9787 - accuracy: 0.6277 - 17ms/epoch - 6ms/step\n",
      "Epoch 71/200\n",
      "3/3 - 0s - loss: 1.9503 - accuracy: 0.6596 - 0s/epoch - 0s/step\n",
      "Epoch 72/200\n",
      "3/3 - 0s - loss: 1.9201 - accuracy: 0.6702 - 15ms/epoch - 5ms/step\n",
      "Epoch 73/200\n",
      "3/3 - 0s - loss: 1.8956 - accuracy: 0.6702 - 0s/epoch - 0s/step\n",
      "Epoch 74/200\n",
      "3/3 - 0s - loss: 1.8645 - accuracy: 0.6915 - 14ms/epoch - 5ms/step\n",
      "Epoch 75/200\n",
      "3/3 - 0s - loss: 1.8444 - accuracy: 0.6702 - 2ms/epoch - 669us/step\n",
      "Epoch 76/200\n",
      "3/3 - 0s - loss: 1.8135 - accuracy: 0.7021 - 0s/epoch - 0s/step\n",
      "Epoch 77/200\n",
      "3/3 - 0s - loss: 1.7854 - accuracy: 0.7021 - 4ms/epoch - 1ms/step\n",
      "Epoch 78/200\n",
      "3/3 - 0s - loss: 1.7620 - accuracy: 0.6915 - 0s/epoch - 0s/step\n",
      "Epoch 79/200\n",
      "3/3 - 0s - loss: 1.7371 - accuracy: 0.7128 - 17ms/epoch - 6ms/step\n",
      "Epoch 80/200\n",
      "3/3 - 0s - loss: 1.7085 - accuracy: 0.7128 - 0s/epoch - 0s/step\n",
      "Epoch 81/200\n",
      "3/3 - 0s - loss: 1.6846 - accuracy: 0.7340 - 17ms/epoch - 6ms/step\n",
      "Epoch 82/200\n",
      "3/3 - 0s - loss: 1.6596 - accuracy: 0.7340 - 0s/epoch - 0s/step\n",
      "Epoch 83/200\n",
      "3/3 - 0s - loss: 1.6354 - accuracy: 0.7340 - 14ms/epoch - 5ms/step\n",
      "Epoch 84/200\n",
      "3/3 - 0s - loss: 1.6120 - accuracy: 0.7340 - 1ms/epoch - 404us/step\n",
      "Epoch 85/200\n",
      "3/3 - 0s - loss: 1.5878 - accuracy: 0.7553 - 13ms/epoch - 4ms/step\n",
      "Epoch 86/200\n",
      "3/3 - 0s - loss: 1.5681 - accuracy: 0.7553 - 2ms/epoch - 727us/step\n",
      "Epoch 87/200\n",
      "3/3 - 0s - loss: 1.5425 - accuracy: 0.7766 - 0s/epoch - 0s/step\n",
      "Epoch 88/200\n",
      "3/3 - 0s - loss: 1.5203 - accuracy: 0.7766 - 4ms/epoch - 1ms/step\n",
      "Epoch 89/200\n",
      "3/3 - 0s - loss: 1.4976 - accuracy: 0.7872 - 0s/epoch - 0s/step\n",
      "Epoch 90/200\n",
      "3/3 - 0s - loss: 1.4771 - accuracy: 0.8085 - 17ms/epoch - 6ms/step\n",
      "Epoch 91/200\n",
      "3/3 - 0s - loss: 1.4580 - accuracy: 0.8085 - 0s/epoch - 0s/step\n",
      "Epoch 92/200\n",
      "3/3 - 0s - loss: 1.4334 - accuracy: 0.8085 - 15ms/epoch - 5ms/step\n",
      "Epoch 93/200\n",
      "3/3 - 0s - loss: 1.4172 - accuracy: 0.8298 - 0s/epoch - 0s/step\n",
      "Epoch 94/200\n",
      "3/3 - 0s - loss: 1.3943 - accuracy: 0.8085 - 14ms/epoch - 5ms/step\n",
      "Epoch 95/200\n",
      "3/3 - 0s - loss: 1.3755 - accuracy: 0.8085 - 1ms/epoch - 499us/step\n",
      "Epoch 96/200\n",
      "3/3 - 0s - loss: 1.3534 - accuracy: 0.8298 - 12ms/epoch - 4ms/step\n",
      "Epoch 97/200\n",
      "3/3 - 0s - loss: 1.3356 - accuracy: 0.8404 - 4ms/epoch - 1ms/step\n",
      "Epoch 98/200\n",
      "3/3 - 0s - loss: 1.3163 - accuracy: 0.8404 - 0s/epoch - 0s/step\n",
      "Epoch 99/200\n",
      "3/3 - 0s - loss: 1.2976 - accuracy: 0.8404 - 5ms/epoch - 2ms/step\n",
      "Epoch 100/200\n",
      "3/3 - 0s - loss: 1.2804 - accuracy: 0.8298 - 0s/epoch - 0s/step\n",
      "Epoch 101/200\n",
      "3/3 - 0s - loss: 1.2607 - accuracy: 0.8404 - 17ms/epoch - 6ms/step\n",
      "Epoch 102/200\n",
      "3/3 - 0s - loss: 1.2433 - accuracy: 0.8404 - 0s/epoch - 0s/step\n",
      "Epoch 103/200\n",
      "3/3 - 0s - loss: 1.2274 - accuracy: 0.8511 - 15ms/epoch - 5ms/step\n",
      "Epoch 104/200\n",
      "3/3 - 0s - loss: 1.2072 - accuracy: 0.8404 - 0s/epoch - 0s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200\n",
      "3/3 - 0s - loss: 1.1937 - accuracy: 0.8511 - 14ms/epoch - 5ms/step\n",
      "Epoch 106/200\n",
      "3/3 - 0s - loss: 1.1739 - accuracy: 0.8511 - 2ms/epoch - 595us/step\n",
      "Epoch 107/200\n",
      "3/3 - 0s - loss: 1.1571 - accuracy: 0.8617 - 12ms/epoch - 4ms/step\n",
      "Epoch 108/200\n",
      "3/3 - 0s - loss: 1.1408 - accuracy: 0.8617 - 2ms/epoch - 568us/step\n",
      "Epoch 109/200\n",
      "3/3 - 0s - loss: 1.1261 - accuracy: 0.8511 - 0s/epoch - 0s/step\n",
      "Epoch 110/200\n",
      "3/3 - 0s - loss: 1.1103 - accuracy: 0.8511 - 17ms/epoch - 6ms/step\n",
      "Epoch 111/200\n",
      "3/3 - 0s - loss: 1.0937 - accuracy: 0.8617 - 0s/epoch - 0s/step\n",
      "Epoch 112/200\n",
      "3/3 - 0s - loss: 1.0812 - accuracy: 0.8617 - 16ms/epoch - 5ms/step\n",
      "Epoch 113/200\n",
      "3/3 - 0s - loss: 1.0604 - accuracy: 0.8617 - 0s/epoch - 0s/step\n",
      "Epoch 114/200\n",
      "3/3 - 0s - loss: 1.0483 - accuracy: 0.8617 - 16ms/epoch - 5ms/step\n",
      "Epoch 115/200\n",
      "3/3 - 0s - loss: 1.0330 - accuracy: 0.8617 - 0s/epoch - 0s/step\n",
      "Epoch 116/200\n",
      "3/3 - 0s - loss: 1.0182 - accuracy: 0.8617 - 14ms/epoch - 5ms/step\n",
      "Epoch 117/200\n",
      "3/3 - 0s - loss: 1.0061 - accuracy: 0.8617 - 2ms/epoch - 592us/step\n",
      "Epoch 118/200\n",
      "3/3 - 0s - loss: 0.9886 - accuracy: 0.8617 - 0s/epoch - 0s/step\n",
      "Epoch 119/200\n",
      "3/3 - 0s - loss: 0.9740 - accuracy: 0.8617 - 3ms/epoch - 940us/step\n",
      "Epoch 120/200\n",
      "3/3 - 0s - loss: 0.9621 - accuracy: 0.8617 - 0s/epoch - 0s/step\n",
      "Epoch 121/200\n",
      "3/3 - 0s - loss: 0.9488 - accuracy: 0.8617 - 17ms/epoch - 6ms/step\n",
      "Epoch 122/200\n",
      "3/3 - 0s - loss: 0.9347 - accuracy: 0.8617 - 0s/epoch - 0s/step\n",
      "Epoch 123/200\n",
      "3/3 - 0s - loss: 0.9209 - accuracy: 0.8617 - 17ms/epoch - 6ms/step\n",
      "Epoch 124/200\n",
      "3/3 - 0s - loss: 0.9086 - accuracy: 0.8723 - 0s/epoch - 0s/step\n",
      "Epoch 125/200\n",
      "3/3 - 0s - loss: 0.8946 - accuracy: 0.8723 - 17ms/epoch - 6ms/step\n",
      "Epoch 126/200\n",
      "3/3 - 0s - loss: 0.8832 - accuracy: 0.8723 - 0s/epoch - 0s/step\n",
      "Epoch 127/200\n",
      "3/3 - 0s - loss: 0.8700 - accuracy: 0.8723 - 15ms/epoch - 5ms/step\n",
      "Epoch 128/200\n",
      "3/3 - 0s - loss: 0.8594 - accuracy: 0.8830 - 924us/epoch - 308us/step\n",
      "Epoch 129/200\n",
      "3/3 - 0s - loss: 0.8453 - accuracy: 0.8830 - 0s/epoch - 0s/step\n",
      "Epoch 130/200\n",
      "3/3 - 0s - loss: 0.8371 - accuracy: 0.8830 - 4ms/epoch - 1ms/step\n",
      "Epoch 131/200\n",
      "3/3 - 0s - loss: 0.8236 - accuracy: 0.9043 - 0s/epoch - 0s/step\n",
      "Epoch 132/200\n",
      "3/3 - 0s - loss: 0.8128 - accuracy: 0.9149 - 16ms/epoch - 5ms/step\n",
      "Epoch 133/200\n",
      "3/3 - 0s - loss: 0.8000 - accuracy: 0.9149 - 0s/epoch - 0s/step\n",
      "Epoch 134/200\n",
      "3/3 - 0s - loss: 0.7904 - accuracy: 0.8936 - 17ms/epoch - 6ms/step\n",
      "Epoch 135/200\n",
      "3/3 - 0s - loss: 0.7782 - accuracy: 0.9149 - 0s/epoch - 0s/step\n",
      "Epoch 136/200\n",
      "3/3 - 0s - loss: 0.7688 - accuracy: 0.9149 - 16ms/epoch - 5ms/step\n",
      "Epoch 137/200\n",
      "3/3 - 0s - loss: 0.7566 - accuracy: 0.9149 - 0s/epoch - 0s/step\n",
      "Epoch 138/200\n",
      "3/3 - 0s - loss: 0.7466 - accuracy: 0.9149 - 13ms/epoch - 4ms/step\n",
      "Epoch 139/200\n",
      "3/3 - 0s - loss: 0.7351 - accuracy: 0.9149 - 2ms/epoch - 599us/step\n",
      "Epoch 140/200\n",
      "3/3 - 0s - loss: 0.7252 - accuracy: 0.9149 - 0s/epoch - 0s/step\n",
      "Epoch 141/200\n",
      "3/3 - 0s - loss: 0.7152 - accuracy: 0.9149 - 17ms/epoch - 6ms/step\n",
      "Epoch 142/200\n",
      "3/3 - 0s - loss: 0.7052 - accuracy: 0.9149 - 0s/epoch - 0s/step\n",
      "Epoch 143/200\n",
      "3/3 - 0s - loss: 0.6958 - accuracy: 0.9255 - 17ms/epoch - 6ms/step\n",
      "Epoch 144/200\n",
      "3/3 - 0s - loss: 0.6860 - accuracy: 0.9255 - 0s/epoch - 0s/step\n",
      "Epoch 145/200\n",
      "3/3 - 0s - loss: 0.6775 - accuracy: 0.9362 - 16ms/epoch - 5ms/step\n",
      "Epoch 146/200\n",
      "3/3 - 0s - loss: 0.6680 - accuracy: 0.9362 - 0s/epoch - 0s/step\n",
      "Epoch 147/200\n",
      "3/3 - 0s - loss: 0.6579 - accuracy: 0.9362 - 14ms/epoch - 5ms/step\n",
      "Epoch 148/200\n",
      "3/3 - 0s - loss: 0.6490 - accuracy: 0.9468 - 1ms/epoch - 405us/step\n",
      "Epoch 149/200\n",
      "3/3 - 0s - loss: 0.6401 - accuracy: 0.9362 - 0s/epoch - 0s/step\n",
      "Epoch 150/200\n",
      "3/3 - 0s - loss: 0.6314 - accuracy: 0.9468 - 2ms/epoch - 829us/step\n",
      "Epoch 151/200\n",
      "3/3 - 0s - loss: 0.6229 - accuracy: 0.9468 - 0s/epoch - 0s/step\n",
      "Epoch 152/200\n",
      "3/3 - 0s - loss: 0.6131 - accuracy: 0.9468 - 17ms/epoch - 6ms/step\n",
      "Epoch 153/200\n",
      "3/3 - 0s - loss: 0.6059 - accuracy: 0.9468 - 0s/epoch - 0s/step\n",
      "Epoch 154/200\n",
      "3/3 - 0s - loss: 0.5978 - accuracy: 0.9574 - 17ms/epoch - 6ms/step\n",
      "Epoch 155/200\n",
      "3/3 - 0s - loss: 0.5895 - accuracy: 0.9574 - 0s/epoch - 0s/step\n",
      "Epoch 156/200\n",
      "3/3 - 0s - loss: 0.5831 - accuracy: 0.9468 - 15ms/epoch - 5ms/step\n",
      "Epoch 157/200\n",
      "3/3 - 0s - loss: 0.5740 - accuracy: 0.9468 - 410us/epoch - 137us/step\n",
      "Epoch 158/200\n",
      "3/3 - 0s - loss: 0.5663 - accuracy: 0.9574 - 13ms/epoch - 4ms/step\n",
      "Epoch 159/200\n",
      "3/3 - 0s - loss: 0.5576 - accuracy: 0.9681 - 2ms/epoch - 718us/step\n",
      "Epoch 160/200\n",
      "3/3 - 0s - loss: 0.5512 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 161/200\n",
      "3/3 - 0s - loss: 0.5421 - accuracy: 0.9681 - 17ms/epoch - 6ms/step\n",
      "Epoch 162/200\n",
      "3/3 - 0s - loss: 0.5371 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 163/200\n",
      "3/3 - 0s - loss: 0.5302 - accuracy: 0.9681 - 17ms/epoch - 6ms/step\n",
      "Epoch 164/200\n",
      "3/3 - 0s - loss: 0.5213 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 165/200\n",
      "3/3 - 0s - loss: 0.5140 - accuracy: 0.9681 - 15ms/epoch - 5ms/step\n",
      "Epoch 166/200\n",
      "3/3 - 0s - loss: 0.5077 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 167/200\n",
      "3/3 - 0s - loss: 0.4998 - accuracy: 0.9681 - 14ms/epoch - 5ms/step\n",
      "Epoch 168/200\n",
      "3/3 - 0s - loss: 0.4931 - accuracy: 0.9681 - 1ms/epoch - 451us/step\n",
      "Epoch 169/200\n",
      "3/3 - 0s - loss: 0.4868 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 170/200\n",
      "3/3 - 0s - loss: 0.4810 - accuracy: 0.9681 - 2ms/epoch - 796us/step\n",
      "Epoch 171/200\n",
      "3/3 - 0s - loss: 0.4742 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 172/200\n",
      "3/3 - 0s - loss: 0.4673 - accuracy: 0.9681 - 17ms/epoch - 6ms/step\n",
      "Epoch 173/200\n",
      "3/3 - 0s - loss: 0.4619 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 174/200\n",
      "3/3 - 0s - loss: 0.4560 - accuracy: 0.9681 - 17ms/epoch - 6ms/step\n",
      "Epoch 175/200\n",
      "3/3 - 0s - loss: 0.4494 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 176/200\n",
      "3/3 - 0s - loss: 0.4442 - accuracy: 0.9681 - 16ms/epoch - 5ms/step\n",
      "Epoch 177/200\n",
      "3/3 - 0s - loss: 0.4378 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 178/200\n",
      "3/3 - 0s - loss: 0.4322 - accuracy: 0.9681 - 16ms/epoch - 5ms/step\n",
      "Epoch 179/200\n",
      "3/3 - 0s - loss: 0.4268 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 180/200\n",
      "3/3 - 0s - loss: 0.4209 - accuracy: 0.9681 - 14ms/epoch - 5ms/step\n",
      "Epoch 181/200\n",
      "3/3 - 0s - loss: 0.4159 - accuracy: 0.9681 - 2ms/epoch - 524us/step\n",
      "Epoch 182/200\n",
      "3/3 - 0s - loss: 0.4103 - accuracy: 0.9681 - 18ms/epoch - 6ms/step\n",
      "Epoch 183/200\n",
      "3/3 - 0s - loss: 0.4049 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 184/200\n",
      "3/3 - 0s - loss: 0.4001 - accuracy: 0.9681 - 15ms/epoch - 5ms/step\n",
      "Epoch 185/200\n",
      "3/3 - 0s - loss: 0.3946 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 186/200\n",
      "3/3 - 0s - loss: 0.3901 - accuracy: 0.9681 - 14ms/epoch - 5ms/step\n",
      "Epoch 187/200\n",
      "3/3 - 0s - loss: 0.3847 - accuracy: 0.9681 - 2ms/epoch - 549us/step\n",
      "Epoch 188/200\n",
      "3/3 - 0s - loss: 0.3797 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 189/200\n",
      "3/3 - 0s - loss: 0.3754 - accuracy: 0.9681 - 4ms/epoch - 1ms/step\n",
      "Epoch 190/200\n",
      "3/3 - 0s - loss: 0.3705 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 191/200\n",
      "3/3 - 0s - loss: 0.3657 - accuracy: 0.9681 - 17ms/epoch - 6ms/step\n",
      "Epoch 192/200\n",
      "3/3 - 0s - loss: 0.3615 - accuracy: 0.9681 - 0s/epoch - 0s/step\n",
      "Epoch 193/200\n",
      "3/3 - 0s - loss: 0.3572 - accuracy: 0.9681 - 16ms/epoch - 5ms/step\n",
      "Epoch 194/200\n",
      "3/3 - 0s - loss: 0.3525 - accuracy: 0.9787 - 0s/epoch - 0s/step\n",
      "Epoch 195/200\n",
      "3/3 - 0s - loss: 0.3484 - accuracy: 0.9787 - 15ms/epoch - 5ms/step\n",
      "Epoch 196/200\n",
      "3/3 - 0s - loss: 0.3438 - accuracy: 0.9787 - 706us/epoch - 235us/step\n",
      "Epoch 197/200\n",
      "3/3 - 0s - loss: 0.3398 - accuracy: 0.9787 - 13ms/epoch - 4ms/step\n",
      "Epoch 198/200\n",
      "3/3 - 0s - loss: 0.3357 - accuracy: 0.9787 - 3ms/epoch - 898us/step\n",
      "Epoch 199/200\n",
      "3/3 - 0s - loss: 0.3320 - accuracy: 0.9787 - 0s/epoch - 0s/step\n",
      "Epoch 200/200\n",
      "3/3 - 0s - loss: 0.3277 - accuracy: 0.9787 - 17ms/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21afc85ec10>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 20\n",
    "hidden_units = 25\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(SimpleRNN(hidden_units))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "478dd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, tokenizer, current_word, n):\n",
    "    init_word = current_word\n",
    "    sentence = ''\n",
    "    \n",
    "    #n 번 반복\n",
    "    for _ in range(n):\n",
    "        # 현재 단어에 대한 정수 인코딩과 패딩\n",
    "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=8, padding='pre')\n",
    "        \n",
    "        # 입력한 x(현재 단어)에 대해서 y를 예측하고 y(에측한단어)를 result에저장\n",
    "        result = model.predict(encoded, verbose=0)\n",
    "        result = np.argmax(result, axis=1)\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\n",
    "            if index == result:\n",
    "                break\n",
    "                \n",
    "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        current_word = current_word + ' ' + word\n",
    "        \n",
    "        # 예측 단어를 문장에 저장\n",
    "        sentence = sentence + ' ' + word\n",
    "        \n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9fb30530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저기 저 별은 나의 마음을 알까 나의 꿈을 알까\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, tokenizer, '저기', 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a6e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e247c909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05ae5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
